{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coronary Heart Disease Prediction: Model Training MLOps Script\n",
    "\n",
    "Section 8 in this notebook will be leveraged as a .py script in the DevOps pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure subscription\n",
    "subscription_id = \"<enter this>\" \n",
    "\n",
    "# Resource Group \n",
    "resource_group = \"mlops-rg\" \n",
    "\n",
    "# Workspace Name and Azure Region of the Azure Machine Learning Workspace\n",
    "workspace_name = \"mlops-aml-ws\" \n",
    "workspace_region = \"eastus2\" \n",
    "\n",
    "# Other variables\n",
    "experiment_name = 'chd-prediction'\n",
    "project_dir = './chd'\n",
    "deployment_dir = './deploy'\n",
    "model_name = 'chd-predictor'\n",
    "model_description = 'Model to predict coronory heart disease'\n",
    "\n",
    "# AML managed compute to be spun up for training\n",
    "vm_name = \"chd-temp-compute\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.model import Model\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Azure Machine Learning Workspace configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an AML workspace - leverage existing, if not - create new\n",
    "ws = Workspace.create(\n",
    "    name = workspace_name,\n",
    "    subscription_id = subscription_id,\n",
    "    resource_group = resource_group, \n",
    "    location = workspace_region,\n",
    "    exist_ok = True) #Leverage existing\n",
    "\n",
    "ws.write_config()\n",
    "print('Workspace configuration succeeded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Experiment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an experiment in the AML workspace\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Project directory configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project directory\n",
    "if not os.path.exists(project_dir):\n",
    "    os.makedirs(project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Azure Machine Learning Managed Compute configuration & provisioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provision AML managed compute \n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=vm_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D12_V2',\n",
    "                                                           min_nodes=1, max_nodes=1)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, vm_name, compute_config)\n",
    "    # Show output\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Environment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Docker based environment with scikit-learn installed\n",
    "training_venv = Environment(\"training_venv\")\n",
    "\n",
    "training_venv.docker.enabled = True\n",
    "training_venv.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Create training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_dir/train.py\n",
    "\n",
    "# Load necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Read training dataset into pandas dataframe\n",
    "dataset_url = ('https://mlopssa.blob.core.windows.net/chd-dataset/framingham.csv')\n",
    "df = pd.read_csv(dataset_url)\n",
    "\n",
    "# create a boolean array of smokers\n",
    "smoke = (df['currentSmoker']==1)\n",
    "# Apply mean to NaNs in cigsPerDay but using a set of smokers only\n",
    "df.loc[smoke,'cigsPerDay'] = df.loc[smoke,'cigsPerDay'].fillna(df.loc[smoke,'cigsPerDay'].mean())\n",
    "\n",
    "# Fill out missing values\n",
    "df['BPMeds'].fillna(0, inplace = True)\n",
    "df['glucose'].fillna(df.glucose.mean(), inplace = True)\n",
    "df['totChol'].fillna(df.totChol.mean(), inplace = True)\n",
    "df['education'].fillna(1, inplace = True)\n",
    "df['BMI'].fillna(df.BMI.mean(), inplace = True)\n",
    "df['heartRate'].fillna(df.heartRate.mean(), inplace = True)\n",
    "\n",
    "# Features and label\n",
    "features = df.iloc[:,:-1]\n",
    "result = df.iloc[:,-1] # the last column is what we are about to forecast\n",
    "\n",
    "# Train & Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, result, test_size = 0.2, random_state = 14)\n",
    "\n",
    "# RandomForest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Create a selector object that will use the random forest classifier to identify\n",
    "# features that have an importance of more than 0.12\n",
    "sfm = SelectFromModel(clf, threshold=0.12)\n",
    "\n",
    "# Train the selector\n",
    "sfm.fit(X_train, y_train)\n",
    "\n",
    "# Features selected\n",
    "feat_labels = list(features.columns.values) # creating a list with features' names\n",
    "for feature_list_index in sfm.get_support(indices=True):\n",
    "    print(feat_labels[feature_list_index])\n",
    "\n",
    "# Feature importance\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# With only imporant features. Can check X_important_train.shape[1]\n",
    "X_important_train = sfm.transform(X_train)\n",
    "X_important_test = sfm.transform(X_test)\n",
    "\n",
    "clf_important = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)\n",
    "clf_important.fit(X_important_train, y_train)\n",
    "\n",
    "# Save the model to disk\n",
    "os.makedirs('./outputs/model', exist_ok=True)\n",
    "\n",
    "filename = './outputs/model/chd-rf-model'\n",
    "pickle.dump(clf_important, open(filename, 'wb'))\n",
    "print(\"model saved in ././outputs/model/chd-rf-model folder\")\n",
    "print(\"Saving model completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Start the training experiment using compute and script from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_dir, script='train.py')\n",
    "\n",
    "# Set compute target to the one created in previous step\n",
    "src.run_config.target = compute_target.name\n",
    "\n",
    "# Set environment\n",
    "src.run_config.environment = training_venv\n",
    " \n",
    "run = experiment.submit(config=src)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Poll for training experiment completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Shows output of the run on stdout\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Training experiment status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Check for success; Register model to model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run.get_status() == 'Completed':\n",
    "    print(\"Training completed successfully!\")\n",
    "    model_run = run.register_model(model_name=model_name,  \n",
    "                               model_path=\"././outputs/model/chd-rf-model\",\n",
    "                               tags={\"type\": \"classification\", \"description\": model_description, \"run_id\": run.id})\n",
    "    print(\"Model registered with version number: \", model_run.version)\n",
    "else:\n",
    "    print(\"Training failed!\")\n",
    "    Exception(\"Training failed!\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
